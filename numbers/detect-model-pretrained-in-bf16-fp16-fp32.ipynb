{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7428a31f",
   "metadata": {},
   "source": [
    "#  bf16, fp16 or fp32 Model Pretraining Detection\n",
    "\n",
    "The goal is to autodetect if a model has been trained in bf16, fp16 or fp32 precision. We want this since we know that bf16-pretrained models tend to overflow when consequently finetuned with fp16 (mixed).\n",
    "\n",
    "We know that fp16's max number is `2**16=65536` (`~6.5e04`), so it should be easy to look at the weights and if they are larger than `1e02` (`sqrt(1e04)`) then the model has most likely been trained in other than fp16 precision (mixed or not).\n",
    "\n",
    "Let's write a script to look at the absolute min/max values of any model's weights, apply it to a bunch of models that we have information on how they were trained and find a pattern. \n",
    "\n",
    "I thought that abs min values could give us some info about the precision, but most likely it's the abs max values that are most telling. Let's see.\n",
    "\n",
    "I also added min and max norms, which I see are quite telling as well.\n",
    "\n",
    "**I'm currently needing more public models to get the patterns right. Please help by adding more models that you know how they were trained. Thank you!**\n",
    "\n",
    "You can submit your contribution and/or read the database gathered so far [here](https://discuss.huggingface.co/t/compiling-data-on-how-models-were-pre-trained-fp16-fp32-bf16/5671).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e951074c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import logging\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "028d21bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b70ce36",
   "metadata": {},
   "source": [
    "## Module weights abs min/max analyser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa0b9662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(modules, verbose=True):\n",
    "    \"\"\"\n",
    "    modules is a list of sub-modules to search recursively. \n",
    "    \n",
    "    this can be the whole model, but sometimes only some submodules want to be inspected\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"\\nSearching:\")\n",
    "        print(\"module | params\")\n",
    "    abs_min, abs_max = 1e10, 0\n",
    "    norm_min, norm_max = 1e10, 0\n",
    "    for i,m in enumerate(modules):\n",
    "        for j,p in enumerate(m.parameters(recurse=True)):\n",
    "            p_abs = p.abs()\n",
    "            p_abs_max = p_abs.max().item()\n",
    "            p_abs_min = p_abs.min().item()\n",
    "            if p_abs_min < abs_min: abs_min = p_abs_min\n",
    "            if p_abs_max > abs_max: abs_max = p_abs_max\n",
    "                \n",
    "            p_norm = torch.linalg.norm(p.data)\n",
    "            if p_norm > 0:\n",
    "                if p_norm < norm_min: norm_min = p_norm\n",
    "                if p_norm > norm_max: norm_max = p_norm\n",
    "        if verbose:\n",
    "            print(f\"{i:>6} | {j}\")\n",
    "    return abs_min, abs_max, norm_min, norm_max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3a43e2",
   "metadata": {},
   "source": [
    "the only concern I have here is that some models when trained in mixed precision may have some segment trained in fp32 and may end up with larger weights, though it is very unlikely since these then have to interact with the rest of the system. But more thought is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "332ce2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils.logging import disable_progress_bar\n",
    "disable_progress_bar() # disable tqdm!\n",
    "\n",
    "model = AutoModel.from_pretrained(\"t5-3b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "197c886d",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Searching:\n",
      "module | params\n",
      "     0 | 192\n",
      "     1 | 312\n",
      "\n",
      "Results:\n",
      "abs min   | abs max   | norm min   | norm max\n",
      "1.455e-11 | 6.950e+01 | 5.201e+00 | 2.535e+03\n",
      "\n",
      "Searching:\n",
      "module | params\n",
      "     0 | 508\n",
      "\n",
      "Results:\n",
      "abs min   | abs max   | norm min   | norm max\n",
      "1.455e-11 | 2.340e+02 | 5.201e+00 | 6.349e+04\n"
     ]
    }
   ],
   "source": [
    "# Let's look at t5-small in verbose mode\n",
    "#model = AutoModel.from_pretrained(\"t5-small\")\n",
    "\n",
    "# let's look at just transformer blocks\n",
    "abs_min, abs_max, norm_min, norm_max = analyze([model.encoder.block, model.decoder.block])\n",
    "print(\"\\nResults:\")\n",
    "print(\"abs min   | abs max   | norm min  | norm max\")\n",
    "print(f\"{abs_min:.3e} | {abs_max:.3e} | {norm_min:.3e} | {norm_max:.3e}\")\n",
    "\n",
    "# now the whole model\n",
    "abs_min, abs_max, norm_min, norm_max = analyze([model])\n",
    "print(\"\\nResults:\")\n",
    "print(\"abs min   | abs max   | norm min  | norm max\")\n",
    "print(f\"{abs_min:.3e} | {abs_max:.3e} | {norm_min:.3e} | {norm_max:.3e}\")\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48185d16",
   "metadata": {},
   "source": [
    "## Multiple model weights abs min/max analyser\n",
    "\n",
    "Now let's write a nice wrapper to process many models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bcf8583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def models_analyze(mnames):\n",
    "    transformers.logging.set_verbosity_error() # be quiet\n",
    "    print(f\"{'name':^40} | {'abs min':^9} | {'abs max':^9} | {'norm min':^9} | {'norm max':^9}  \")\n",
    "    print(f\"{'-'*40}-|-{'-'*9}-|-{'-'*9}-|-{'-'*9}-|-{'-'*9}-\")\n",
    "    for mname in mnames:\n",
    "        model = AutoModel.from_pretrained(mname)\n",
    "        abs_min, abs_max, norm_min, norm_max = analyze([model], verbose=False)\n",
    "        print(f\"{mname:<40} | {abs_min:.3e} | {abs_max:.3e} | {norm_min:.3e} | {norm_max:.3e}\")\n",
    "        del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f92dc2",
   "metadata": {},
   "source": [
    "## fp16 models\n",
    "\n",
    "Let's look at fp16-pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a74dfcdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  name                   |  abs min  |  abs max  | norm min  | norm max   \n",
      "-----------------------------------------|-----------|-----------|-----------|-----------\n",
      "allenai/longformer-base-4096             | 0.000e+00 | 1.510e+00 | 2.272e-02 | 7.993e+02\n",
      "allenai/longformer-large-4096            | 0.000e+00 | 1.146e+00 | 9.087e-02 | 9.428e+02\n",
      "allenai/led-base-16384                   | 0.000e+00 | 1.600e+01 | 1.611e-02 | 4.147e+02\n",
      "allenai/led-large-16384                  | 0.000e+00 | 2.320e+01 | 4.799e-02 | 6.362e+02\n",
      "lvwerra/codeparrot                       | 1.245e-11 | 1.832e+00 | 1.185e-01 | 2.112e+02\n",
      "facebook/m2m100_418M                     | 0.000e+00 | 1.000e+00 | 4.792e-01 | 4.829e+02\n",
      "facebook/m2m100_1.2B                     | 0.000e+00 | 1.000e+00 | 4.835e-01 | 4.925e+02\n",
      "facebook/opt-1.3b                        | 0.000e+00 | 1.000e+00 | 4.852e-02 | 3.619e+02\n",
      "facebook/opt-13b                         | 0.000e+00 | 1.000e+00 | 7.830e-02 | 3.136e+02\n",
      "bigscience/bloom-7b1                     | 0.000e+00 | 1.783e+01 | 1.645e+00 | 2.669e+02\n",
      "bigscience/bloom-3b                      | 0.000e+00 | 2.522e+01 | 1.406e+00 | 2.606e+02\n"
     ]
    }
   ],
   "source": [
    "# fp16-pretrained models\n",
    "mnames = [\"allenai/longformer-base-4096\", \"allenai/longformer-large-4096\", \n",
    "          \"allenai/led-base-16384\", \"allenai/led-large-16384\", \"lvwerra/codeparrot\", \n",
    "          \"facebook/m2m100_418M\", \"facebook/m2m100_1.2B\",\n",
    "           \"facebook/opt-1.3b\", \"facebook/opt-13b\",\n",
    "           \"bigscience/bloom-7b1\", \"bigscience/bloom-3b\",         \n",
    "         ]\n",
    "models_analyze(mnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3500cd2b",
   "metadata": {},
   "source": [
    "So we can see the fp16 abs max weights are quite small - they are in the range of 1e0 - 1e1.\n",
    "\n",
    "The norm max is also always under 1e3 in our samples\n",
    "\n",
    "abs max for \"led\" models is oddly pretty high. They are supposed to be the same as longformer, which are fp16. But norm max matches other models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b266fb1a",
   "metadata": {},
   "source": [
    "## bf16 models\n",
    "\n",
    "Let's look at bf16-pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1479a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  name                   |  abs min  |  abs max  | norm min  | norm max   \n",
      "-----------------------------------------|-----------|-----------|-----------|-----------\n",
      "t5-small                                 | 5.442e-09 | 7.920e+02 | 1.780e+00 | 9.403e+04\n",
      "t5-base                                  | 1.273e-10 | 5.600e+02 | 1.647e+00 | 9.332e+04\n",
      "t5-large                                 | 3.638e-11 | 5.200e+02 | 3.797e+00 | 8.237e+04\n",
      "google/mt5-small                         | 3.201e-09 | 1.140e+02 | 2.662e+00 | 1.610e+05\n",
      "google/mt5-base                          | 1.848e-09 | 1.135e+02 | 3.445e+00 | 1.639e+05\n",
      "google/mt5-large                         | 1.892e-10 | 1.750e+02 | 4.472e+00 | 2.029e+05\n",
      "google/bigbird-pegasus-large-arxiv       | 0.000e+00 | 2.424e+02 | 4.955e-01 | 3.183e+03\n",
      "google/pegasus-cnn_dailymail             | 0.000e+00 | 2.416e+02 | 4.926e-01 | 4.423e+03\n",
      "google/pegasus-large                     | 0.000e+00 | 2.417e+02 | 4.912e-01 | 4.745e+03\n",
      "google/pegasus-multi_news                | 0.000e+00 | 2.412e+02 | 4.925e-01 | 4.377e+03\n",
      "google/pegasus-xsum                      | 0.000e+00 | 2.418e+02 | 4.914e-01 | 4.402e+03\n",
      "bigscience/T0_3B                         | 5.755e-13 | 1.680e+02 | 3.114e+00 | 7.432e+04\n",
      "EleutherAI/gpt-neo-1.3B                  | 2.456e-10 | 5.125e+00 | 1.337e+00 | 1.055e+03\n"
     ]
    }
   ],
   "source": [
    "# bf16-pretrained models\n",
    "mnames = [\"t5-small\", \"t5-base\", \"t5-large\", \"google/mt5-small\", \"google/mt5-base\", \n",
    "          \"google/mt5-large\",\n",
    "          \"google/bigbird-pegasus-large-arxiv\", \"google/pegasus-cnn_dailymail\", \n",
    "          \"google/pegasus-large\", \"google/pegasus-multi_news\", \"google/pegasus-xsum\",\n",
    "          \"bigscience/T0_3B\", \"EleutherAI/gpt-neo-1.3B\",\n",
    "]\n",
    "# \"bigscience/T0pp\", T0 are huge!\n",
    "models_analyze(mnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31280426",
   "metadata": {},
   "source": [
    "We can see big abs max weight values - pretty consistently - so perhaps if the max weight > 1e2 it's a good candidate for bf16 group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33149dff",
   "metadata": {},
   "source": [
    "## fp32 models\n",
    "\n",
    "Let's look at fp32-pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "517e0226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  name                   |  abs min  |  abs max  | norm min  | norm max   \n",
      "-----------------------------------------|-----------|-----------|-----------|-----------\n",
      "gsarti/it5-small                         | 6.114e-08 | 4.693e+02 | 8.411e-02 | 6.881e+04\n",
      "gsarti/it5-base                          | 1.068e-08 | 1.598e+03 | 3.596e-01 | 8.997e+04\n",
      "gsarti/it5-base-oscar                    | 3.638e-12 | 2.092e+01 | 3.637e+00 | 5.758e+03\n",
      "gsarti/it5-large                         | 2.094e-09 | 4.388e+04 | 7.982e-02 | 1.105e+06\n",
      "EleutherAI/gpt-neo-2.7B                  | 2.319e-11 | 3.563e+00 | 1.322e+00 | 9.850e+02\n"
     ]
    }
   ],
   "source": [
    "# fp32-pretrained models\n",
    "mnames = [\"gsarti/it5-small\", \"gsarti/it5-base\", \"gsarti/it5-base-oscar\", \n",
    "          \"gsarti/it5-large\", \"EleutherAI/gpt-neo-2.7B\", \n",
    "         ]\n",
    "models_analyze(mnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081c1126",
   "metadata": {},
   "source": [
    "The abs max is all over the map here.\n",
    "\n",
    "\"EleutherAI/gpt-neo-2.7B\"'s abs max is very low.\n",
    "\n",
    "XXX: need more inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a447e780",
   "metadata": {},
   "source": [
    "## Unknown models\n",
    "\n",
    "Let's look at some uknown models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6e8df28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fp32? (XXX: need to check)\n",
    "#mnames = [\"bigscience/T0_3B\"] \n",
    "# mnames = [\"bigscience/T0pp\", \"bigscience/T0_3B\"] \"bigscience/T0pp\" is huge!\n",
    "#models_analyze(mnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bb4a50",
   "metadata": {},
   "source": [
    "need to check how it was trained - looks like bf16 to me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16f37c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fp32? (XXX: need to check)\n",
    "#mnames = [\"google/pegasus-pubmed\"] \n",
    "#mnames = [] \n",
    "\n",
    "#mnames = [\"\"] \n",
    "#models_analyze(mnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af36a2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "263.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
